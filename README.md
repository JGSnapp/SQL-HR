# SQL-HR

## Описание и назначение системы
SQL-HR автоматизирует отбор релевантных кандидатов по базе резюме. Система разворачивается как набор Docker‑сервисов: PostgreSQL хранит данные, `agent_server` строит пайплайн LangGraph и взаимодействует с LLM через vLLM, а результат сохраняется в `results/result.txt`. Подход позволяет итеративно уточнять поисковые акценты, дозапрашивать новых кандидатов и ранжировать их в соответствии с бизнес‑запросом. Проект изначально использует Qwen2.5‑7B‑GPTQ.

## Структура проекта
- `docker-compose.yml` — единая оркестрация для vLLM, `agent_server`, PostgreSQL и pgAdmin, а также проброс каталога `results/` внутрь контейнера.
- `env.example` — пример переменных окружения (доступ к БД, порты, модель LLM и ключ API).
- `agent_server/` — исходный код пайплайна (главные файлы `main.py`, `nodes.py`, `candidates.py`, `prompts.py`, `requirements.txt`, `Dockerfile`).
- `data/` — файлы с исходными резюме, загружается в PostgreSQL при старте.
- `db/schema.sql`, `db/add_rows.sql` — описание таблицы `candidates` и импорт исходных данных.
- `hf-cache/` — кеш Hugging Face, подключается к vLLM для повторного использования моделей.
- `results/result.txt` — итоговый JSON со списком отобранных кандидатов (создаётся автоматически).
- `.gitignore`, `Dockerfile` (в корне) и вспомогательные файлы для локальной разработки.

## Этапы отбора (по нодам LangGraph)
1. `node_test_db` — быстрая проверка подключения к PostgreSQL: выбирает пять записей и логирует их.
2. `node_get_task` — запрашивает у LLM текстовую постановку поиска (например, какая вакансия нужна).
3. `node_generate_accents` — формирует 1–8 тематических «акцентов» (вариантов запросов) из исходной задачи.
4. `node_choose_candidates` — для каждого акцента LLM строит первичный `QuerySpec`, по которому ORM извлекает кандидатов; затем LLM помечает наиболее подходящих.
5. `node_add_candidates` — обеспечивает целевое количество кандидатов в каждой группе: повторно создаёт уточнённые запросы, при необходимости «смягчает» фильтры и отдает LLM на повторный выбор.
6. `node_rate_candidates` — собирает все уникальные резюме, объединяет флаги `approved` и просит LLM выдать ранжированный список (есть детерминированный fallback).
7. `node_return_candidates` — сериализует результат в JSON и пытается записать его в `RESULT_FILE` (по умолчанию `results/result.txt`). Если основной путь недоступен, используется `/tmp/result.txt` внутри контейнера.
8. `node_ask_next` — задаёт пользователю вопрос для следующего уточнения и завершает текущий запуск.

## Установка и эксплуатация
1. **Зависимости.** Нужны Docker и Docker Compose. Чтобы vLLM работал, требуется GPU.
2. **Подготовка окружения.**
   ```bash
   cp env.example .env
   # отредактируйте .env: пароли БД, порт pgAdmin, модель/ключ LLM и т.д.
   ```
3. **Запуск сервисов.**
   ```bash
   docker compose up --build
   ```
   - `postgres` поднимет схему и загрузит `data/candidates.csv` через `db/*.sql`.
   - `vllm` скачает модель в `hf-cache/` и откроет API на `${VLLM_HOST_PORT}` (по умолчанию 18001).
   - `agent_server` соберёт LangGraph пайплайн и запишет результат в `results/result.txt` (также доступен внутри контейнера как `RESULT_FILE`).  
   - `pgadmin` будет доступен на `${PGADMIN_PORT}` (например, http://localhost:5050) — можно подключиться к БД и проверить данные.
4. **Получение результата.**
   После завершения пайплайна файл `results/result.txt` на хосте содержит структуру:
   ```json
   {
     "total": 10,
     "candidates": [
       {
         "id": "uuid",
         "approved": true,
         "desired_position": "...",
         "city": "...",
         "expected_salary_rub": 250000,
         "...": "..."
       }
     ]
   }
   ```